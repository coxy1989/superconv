{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Dict, Callable\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  864/60000 [..............................] - ETA: 6:31 - loss: 2.2820 - acc: 0.1250"
     ]
    }
   ],
   "source": [
    "# NUMPY -> KERAS\n",
    "\n",
    "def gen_model():\n",
    "    'TODO: docstring'    \n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(input_shape=(28, 28, 1),\n",
    "                               filters=20,\n",
    "                               kernel_size=5),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "        tf.keras.layers.Conv2D(filters=50,\n",
    "                               kernel_size=5),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(500, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')])\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = (x_train / 255.0).astype(np.float32), (x_test / 255.0).astype(np.float32)\n",
    "y_train, y_test = y_train.astype(np.float32), y_test.astype(np.float32)\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "model = gen_model()\n",
    "opt = tf.keras.optimizers.SGD(0.01)\n",
    "model.compile(opt,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x=x_train, y=y_train, epochs=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DATASET -> KERAS\n",
    "\n",
    "def gen_model():\n",
    "    'TODO: docstring'    \n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.Conv2D(filters=20,\n",
    "                               kernel_size=5),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "        tf.keras.layers.Conv2D(filters=50,\n",
    "                               kernel_size=5),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(500, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')])\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), _ = mnist.load_data()\n",
    "x_train = (x_train / 255.0).astype(np.float32)[..., tf.newaxis]\n",
    "y_train = tf.one_hot(y_train, 10)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(100).batch(32)\n",
    "train_it = train_ds.make_one_shot_iterator()\n",
    "xs, ys = train_it.get_next()\n",
    "model = gen_model()\n",
    "opt = tf.keras.optimizers.SGD(0.01)\n",
    "steps = math.floor(x_train.shape[0] / 32)\n",
    "model.compile(opt, \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(xs, ys, epochs=1, steps_per_epoch=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _1cycle(iteration_idx:int,\n",
    "            cyc_iterations:int,\n",
    "            ramp_iterations:int,\n",
    "            min_lr:float,\n",
    "            max_lr:float):\n",
    "    'TODO: docstring'\n",
    "    if cyc_iterations % 2 != 1:\n",
    "        raise ValueError('Even value for `cyc_iterations` implies asymetric step size')\n",
    "    mid = (cyc_iterations - 1)/2\n",
    "    if iteration_idx == mid: return max_lr\n",
    "    elif iteration_idx == 0 or iteration_idx == (2 * mid): return min_lr\n",
    "    elif iteration_idx < cyc_iterations: \n",
    "        mod = (iteration_idx % mid)\n",
    "        numerator =  mod if iteration_idx < mid else mid - mod\n",
    "        return min_lr + (numerator / mid) * (max_lr - min_lr)\n",
    "    else:\n",
    "        idx = iteration_idx - cyc_iterations\n",
    "        ramp_max = min_lr\n",
    "        ramp_min = min_lr * 1e-5\n",
    "        return ramp_max - ((idx + 1) / ramp_iterations) * (ramp_max - ramp_min)\n",
    "\n",
    "class OneCycleSchedulerCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, cyc_iterations:int, ramp_iterations:int, min_lr:float, max_lr:float):\n",
    "        'TODO: docstring'        \n",
    "        self.cyc_iterations = cyc_iterations\n",
    "        self.ramp_iterations = ramp_iterations\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr = min_lr\n",
    "        self.lrs = []\n",
    "        self.its = []\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        'TODO: docstring'        \n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        self.lr = _1cycle_f(logs['batch'] - 1, self.cyc_iterations, self.ramp_iterations, self.min_lr, self.max_lr)\n",
    "        K.set_value(self.model.optimizer.lr, self.lr)\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        'TODO: docstring'\n",
    "        self.lrs.append(self.lr)\n",
    "        self.its.append(logs['batch'])\n",
    "        \n",
    "    def plot_lr_vs_iteration(self):\n",
    "        'TODO: docstring'        \n",
    "        df = pd.DataFrame({'lr' : self.lrs, 'iteration': self.its})\n",
    "        sns.lineplot(x='iteration', y='lr', data=df)\n",
    "        \n",
    "def _triangular_f(it:int, ss:int, min_lr:float, max_lr:float):\n",
    "    'TODO: docstring'\n",
    "    # calculate number of completed cycles\n",
    "    cyc = math.floor(it / (ss * 2))\n",
    "    # calculate number of completed iterations in current cycle\n",
    "    it_cyc = it - (cyc * 2 * ss)\n",
    "    # calculate distance from lr_max iteration\n",
    "    mid_dist = math.fabs(it_cyc - ss)\n",
    "    # scale lr difference\n",
    "    scalar = mid_dist / ss\n",
    "    return min_lr + (1 - scalar) * (max_lr - min_lr)\n",
    "\n",
    "class LRFinderCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, step_size:int, min_lr:float, max_lr:float, evaluate_mod:int, evaluate_fn:Callable):\n",
    "        'TODO: docstring'        \n",
    "        super().__init__()\n",
    "        self.step_size = step_size\n",
    "        self.lr = min_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.evaluate_mod = evaluate_mod\n",
    "        self.evaluate_fn = evaluate_fn\n",
    "        self.lrs = []\n",
    "        self.its = []\n",
    "        self.val_lrs = []\n",
    "        self.val_loss = []\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        'TODO: docstring'\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        self.lr = _triangular_f(logs['batch'], self.step_size, self.min_lr, self.max_lr)\n",
    "        K.set_value(self.model.optimizer.lr, self.lr)\n",
    "                \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        'TODO: docstring'\n",
    "        self.lrs.append(self.lr)\n",
    "        self.its.append(logs['batch'])\n",
    "        if logs['batch'] % self.evaluate_mod == 0:\n",
    "            self.val_lrs.append(self.lr)\n",
    "            self.val_loss.append(self.evaluate_fn())\n",
    "        \n",
    "    def plot_lr_vs_iteration(self):\n",
    "        'TODO: docstring'        \n",
    "        df = pd.DataFrame({'lr' : self.lrs, 'iteration': self.its})\n",
    "        sns.lineplot(x='iteration', y='lr', data=df)\n",
    "        \n",
    "    def plot_lr_vs_val_acc(self):\n",
    "        'TODO: docstring'        \n",
    "        df = pd.DataFrame({'lr' : self.val_lrs, 'loss': self.val_loss})\n",
    "        sns.lineplot(x='lr', y='loss', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 254/1875 [===>..........................] - ETA: 7:39 - loss: 0.4393 - acc: 0.8643"
     ]
    }
   ],
   "source": [
    "def gen_model():\n",
    "    'TODO: docstring'    \n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.Conv2D(filters=20,\n",
    "                               kernel_size=5),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "        tf.keras.layers.Conv2D(filters=50,\n",
    "                               kernel_size=5),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(500, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')])\n",
    "\n",
    "def gen_mnist_iterator(x, y, bs):\n",
    "    'TODO: docstring'    \n",
    "    x = (x / 255.0).astype(np.float32)[..., tf.newaxis]\n",
    "    y = tf.one_hot(y, 10)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y)).shuffle(100).batch(bs)\n",
    "    return ds.make_one_shot_iterator()\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "train_xs, train_ys = gen_mnist_iterator(x_train, y_train, 32).get_next()\n",
    "test_xs, test_ys = gen_mnist_iterator(x_test, y_test, 25).get_next()\n",
    "\n",
    "model = gen_model()\n",
    "opt = tf.keras.optimizers.SGD(0.01)\n",
    "train_steps = math.floor(x_train.shape[0] / 32)\n",
    "test_steps = math.floor(x_test.shape[0] / 25)\n",
    "#cb = LRFinderCallback(step_size=10,\n",
    "#                      min_lr=0.1,\n",
    "#                      max_lr=1,\n",
    "#                      evaluate_mod=10000,\n",
    "#                      evaluate_fn= lambda: model.evaluate(test_xs, test_ys, steps=test_steps))\n",
    "\n",
    "cb = OneCycleSchedulerCallback(cyc_iterations=50000,\n",
    "                               ramp_iterations=10000,\n",
    "                               min_lr=0.1,\n",
    "                               max_lr=1)\n",
    "model.compile(opt, \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_xs, train_ys, epochs=1, steps_per_epoch=train_steps, callbacks=[cb])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
